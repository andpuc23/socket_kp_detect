{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f29d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os, json, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.transforms import functional as F\n",
    "from utils_scripts import utils\n",
    "from utils_scripts.classes.metric_logger import MetricLogger\n",
    "\n",
    "# import transforms, utils, engine, train\n",
    "\n",
    "import copy\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "# import pycocotools.mask as mask_util\n",
    "# from pycocotools.coco import COCO\n",
    "# from pycocotools.cocoeval import COCOeval\n",
    "# from coco_utils import get_coco_api_from_dataset\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63942494",
   "metadata": {},
   "outputs": [],
   "source": [
    "false = False\n",
    "true = True\n",
    "\n",
    "KEYPOINTS_NUM = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b96ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetClass(Dataset):\n",
    "    def __init__(self, \n",
    "                 root:str, # root folder\n",
    "                 annos:str, #annottaions file\n",
    "                 split:list, #mask of test data over all data\n",
    "                 transform=None, demo=False, only_image=False):\n",
    "        self.root = root\n",
    "        with open(annos) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            self.annos = np.asarray(data['annotations'])[split]\n",
    "            self.imgs = np.asarray(\n",
    "                [os.path.join(self.root, img_dict['file_name']) \n",
    "                 for img_dict in np.asarray(data['images'])]\n",
    "            )[split]\n",
    "        self.transform = transform\n",
    "        self.demo = demo\n",
    "        self.only_image = only_image\n",
    "\n",
    "                # delete bad images - we assume all are good\n",
    "#         bad = [\n",
    "#             'content/socket_v2/frame51_augmented_order_32.jpg',\n",
    "#             'content/socket_v2/frame60_augmented_order_8.jpg',\n",
    "#             'content/socket_v2/frame60_augmented_order_22.jpg',\n",
    "#             'content/socket_v2/frame62_augmented_order_37.jpg',\n",
    "#             'content/socket_v2/frame60_augmented_order_6.jpg',\n",
    "#             'content/socket_v2/frame52_augmented_order_15.jpg',\n",
    "#             'content/socket_v2/frame51_augmented_order_2.jpg',\n",
    "#             'content/socket_v2/frame61_augmented_order_5.jpg',\n",
    "#             'content/socket_v2/frame62_augmented_order_0.jpg',\n",
    "#         ]\n",
    "\n",
    "#         for bad_name in bad:\n",
    "#             indx = np.where(self.imgs == bad_name)\n",
    "#             np.delete(self.imgs,indx)\n",
    "#             np.delete(self.annos,indx)\n",
    "        # modify bboxes\n",
    "\n",
    "        self.bboxes = []\n",
    "        for indx in range(len(self.annos)):\n",
    "            bboxes_original = [self.annos[indx]['bbox']]\n",
    "            bboxes_original[0][2] += bboxes_original[0][0]\n",
    "            bboxes_original[0][3] += bboxes_original[0][1]\n",
    "            self.bboxes.append(bboxes_original)\n",
    "\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        if self.only_image:\n",
    "            img_path = self.only_image\n",
    "            idx = list(self.imgs).index(img_path)\n",
    "        else:\n",
    "            img_path = self.imgs[idx]\n",
    "\n",
    "        img_original = cv2.imread(img_path)\n",
    "        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # bboxes_original = [self.annos[idx]['bbox']]\n",
    "        # bboxes_original[0][2]+=bboxes_original[0][0]\n",
    "        # bboxes_original[0][3]+=bboxes_original[0][1]\n",
    "\n",
    "        bboxes_original = self.bboxes[idx]\n",
    "        # All objects are glue tubes\n",
    "        bboxes_labels_original = ['Socket' for _ in bboxes_original]\n",
    "\n",
    "        keypoints_original = self.annos[idx]['keypoints']\n",
    "        keypoints_original = [np.asarray(keypoints_original).reshape(-1, 3)]\n",
    "\n",
    "        # mask = np.array([\n",
    "#         True,True,False,\n",
    "#         True,True,False\n",
    "#         ,True,True,False\n",
    "#         ,True,True,False,\n",
    "#         True,True,False\n",
    "#         ,True,True,False\n",
    "#         ,True,True,False,\n",
    "#         True,True,False,\n",
    "#         True,True,False,\n",
    "#         True,True,False,\n",
    "#         True,True,False\n",
    "#         ,True,True,False\n",
    "#         ,True,True,False])\n",
    "\n",
    "        if self.transform:\n",
    "            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints\n",
    "            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format\n",
    "            # Then we need to convert it to the following list:\n",
    "            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n",
    "            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n",
    "\n",
    "            # Apply augmentations\n",
    "            # try to apply aug:\n",
    "            try:\n",
    "                transformed = self.transform(image=img_original, bboxes=bboxes_original,\n",
    "                                             bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n",
    "                img = transformed['image']\n",
    "                bboxes = transformed['bboxes']\n",
    "                #if bboxes[0] < 1 or bboxes[1] < 1 or bboxes[2] < 1 or bboxes[3] < 1:\n",
    "                #    print(bboxes)\n",
    "                # print(\"transformed['keypoints']\",transformed['keypoints'])\n",
    "                # Unflattening list transformed['keypoints']\n",
    "                # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n",
    "                # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n",
    "                # Then we need to convert it to the following list:\n",
    "                # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n",
    "                #print(f\"'{img_path}',\")\n",
    "                keypoints_transformed_unflattened =\\\n",
    "                    np.reshape(\n",
    "                        np.array(transformed['keypoints']),\n",
    "                        (-1, KEYPOINTS_NUM, 2)\n",
    "                    ).tolist()\n",
    "\n",
    "# Converting transformed keypoints from \n",
    "# [x, y]-format to [x,y,visibility]-format \n",
    "# by appending original visibilities to transformed\n",
    "# coordinates of keypoints\n",
    "                keypoints = []\n",
    "                for o_idx, obj in enumerate(keypoints_transformed_unflattened):  # Iterating over objects\n",
    "                    obj_keypoints = []\n",
    "                    for k_idx, kp in enumerate(obj):  # Iterating over keypoints in each object\n",
    "                        # kp - coordinates of keypoint\n",
    "                        # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n",
    "                        obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n",
    "                    keypoints.append(obj_keypoints)\n",
    "            except Exception as e:\n",
    "                #print(f'Exception {e}. Apply no augmentation to the image')\n",
    "                img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "\n",
    "        else:\n",
    "            #print(bboxes_original)\n",
    "            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original\n",
    "\n",
    "            # Convert everything into a torch tensor\n",
    "        #print('bboxes_original', bboxes_original)\n",
    "        #print('bboxes',bboxes)\n",
    "        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)\n",
    "        #print('bboxes tensor', bboxes)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = bboxes\n",
    "        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64)  # all objects are glue tubes\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n",
    "        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "        img = F.to_tensor(img)\n",
    "\n",
    "        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n",
    "        target_original = {}\n",
    "        target_original[\"boxes\"] = bboxes_original\n",
    "        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original],\n",
    "                                                    dtype=torch.int64)  # all objects are glue tubes\n",
    "        target_original[\"image_id\"] = torch.tensor([idx])\n",
    "        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (\n",
    "                    bboxes_original[:, 2] - bboxes_original[:, 0])\n",
    "        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n",
    "        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)\n",
    "        img_original = F.to_tensor(img_original)\n",
    "        #print(target)\n",
    "        if self.demo:\n",
    "            return img, target, img_original, target_original\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b34572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(num_keypoints=KEYPOINTS_NUM,\n",
    "              weights_path=None,\n",
    "              load_device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'),\n",
    "              pretrained_net=False):\n",
    "    if not pretrained_net and not weights_path:\n",
    "        warnings.warn('model is not pretrained and no state dict provided, the learning process will be long!')\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=(32, 64, 128, 256, 512),\n",
    "        aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0)\n",
    "    )\n",
    "    model = keypointrcnn_resnet50_fpn(pretrained=pretrained_net, \n",
    "                                      pretrained_backbone=True,\n",
    "                                      num_keypoints=num_keypoints,\n",
    "                                      num_classes=2, # Background is the first class, object is the second class\n",
    "                                      rpn_anchor_generator=anchor_generator)\n",
    "\n",
    "    if weights_path:\n",
    "        state_dict = torch.load(weights_path, map_location=load_device)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad499db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SNIPER~1\\AppData\\Local\\Temp/ipykernel_16772/2482049715.py:6: UserWarning: model is not pretrained and no state dict provided, the learning process will be long!\n",
      "  warnings.warn('model is not pretrained and no state dict provided, the learning process will be long!')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]\t[   0/1550]\teta: 8:35:54\tlr: 0.000016\tloss: 9.4578 (9.4578)\tloss_classifier: 0.6875 (0.6875)\tloss_box_reg: 0.0077 (0.0077)\tloss_keypoint: 8.0666 (8.0666)\tloss_objectness: 0.6938 (0.6938)\tloss_rpn_box_reg: 0.0021 (0.0021)\ttime: 19.9710\tdata: 0.1250\n",
      "Epoch: [0]\t[ 100/1550]\teta: 8:46:07\tlr: 0.000815\tloss: 4.3372 (6.8686)\tloss_classifier: 0.0458 (0.2044)\tloss_box_reg: 0.0691 (0.0527)\tloss_keypoint: 4.1883 (6.2106)\tloss_objectness: 0.0185 (0.3937)\tloss_rpn_box_reg: 0.0066 (0.0072)\ttime: 23.6815\tdata: 0.1078\n"
     ]
    }
   ],
   "source": [
    "DATASET_LEN = 3468\n",
    "TRAIN_SIZE = 3100\n",
    "imgs_folder = 'data/big_experiment'\n",
    "annotations_file = 'data/big_experiment/extended_17.json'\n",
    "\n",
    "num_epochs = 450\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# create split\n",
    "split = np.full(DATASET_LEN, False)\n",
    "split[:TRAIN_SIZE] = True\n",
    "np.random.shuffle(split)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "dataset_train = DatasetClass(\n",
    "    root=imgs_folder,\n",
    "    annos=annotations_file,\n",
    "    split=split,\n",
    "    transform=utils.train_transform(), demo=False)\n",
    "\n",
    "dataset_test = DatasetClass(\n",
    "    root=imgs_folder,\n",
    "    annos=annotations_file,\n",
    "    split=~split,\n",
    "    transform=None, demo=False)\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=utils.collate_fn)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "model = get_model(num_keypoints=KEYPOINTS_NUM)#, weights_path=None, pretrained_net=True)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, \n",
    "                            lr=8*1e-3,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=0.00012)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=10,\n",
    "                                               gamma=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    response = utils.train_one_epoch(model,\n",
    "                    optimizer,\n",
    "                    data_loader_train,\n",
    "                    device,\n",
    "                    epoch,\n",
    "                    print_freq=100)\n",
    "    print(f'Epoch {epoch} finished', end='')\n",
    "    lr_scheduler.step()\n",
    "#     utils.evaluate(model, data_loader_test, device)   # TODO VALIDATION LOSS ПОПРАВИТЬ\n",
    "    print(', done validation')\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        torch.save(model.state_dict(), f'outputs/keypointsrcnn_weights_epoch_{epoch}.pth')\n",
    "\n",
    "# Save model weights after training\n",
    "torch.save(model.state_dict(), 'outputs/keypointsrcnn_weights_final.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab9b178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
